{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu soluci√≥n y todas las suposiciones que est√°s considerando. Aqu√≠ puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solucion Challenge Latam\n",
    "## Nelson David Navarro Diaz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota. La explicacion detalla de cada una de las operaciones hara parte de los comentarios de cada funcion, los comentarios se haran en ingles entendiendo las buenas practicas de comentar en este idioma, debido a que este idioma no nos limitaria a compartirlo con personas no hispanohablantes. Este documento si se hace en espanol por ser el idioma de los involucrados en este proceso de seleccion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de inicial el desarrollo de cada uno de los casos aplicando es necesario hacer una revision de los datos que tenemos. Esto debido a que el entender el negocio y las condiciones de los datos que se estaran procesando es el primera paso para desarrollar una solucion.  Ya que el entender el origen, la naturaleza de los datos y la aplicacion que le vamos a dar a estos es lo que finalmente nos permitira tomar las decisiones adecuadas en el desarrollo de una solucion.  En este caso particular tenemos los siguientes insigths. \n",
    "-  La informacion tiene como origen la plataforma X antes twitter y al ser la informacion publicada por estos no solo tiene la informacion necesaria en cada caso, sino que tambien esta cargada de informacion adicional.\n",
    "- Aunque en este momento tenemos un set de datos limitados este podria seguir creciendo indefinidamente ya que esta informacion se genera en tiempo real en esta aplicacion. \n",
    "- La informacion ingresa en un formato Json con Child Json, por lo que alguna informacion no  podra ser leida en el primer nivel de exploracion del Json. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Las top 10 fechas donde hay m√°s tweets. Mencionar el usuario (username) que m√°s publicaciones tiene por cada uno de esos d√≠as. Debe incluir las siguientes funciones:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Optimizacion del tiempo de ejecucion.\n",
    "Incialmente considerando que la informacion viene  en formato Json el primer paso es transformarlo a un tipo de datos mucho mas facil de manejar. En este caso se procede a pasaralo a dataframe de pyspark. Debido a que este formato es columnar lo que al trabajar con grandes datos nos permite utilizar unicamente las colummnas que requerimos lo que hace que se trabaje con menos informacion. En este caso como se busca la optimizacion del tiempo se haran los procesos utilizando la totalidad del archivo  lo que hara que cada una de las operaciones se haga de una vez sobre la totalidad del archivo.\n",
    "\n",
    "AHora procedemos a llamar la funcion, inicialmente medimos tiempo de ejecucion, memoria y vemos el resultado la ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: c:\\Users\\Nelson\\Documents\\latam_challenge\\src\\q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    12     78.6 MiB     78.6 MiB           1   @profile\n",
      "    13                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    14                                                 #El proceso se hace con spark para trabajar la informacion de manera columnar. \n",
      "    15     80.0 MiB      1.4 MiB           1           spark = SparkSession.builder.appName('sparkdf').getOrCreate() \n",
      "    16     80.0 MiB      0.0 MiB           1           df = spark.read.json(file_path)\n",
      "    17     80.0 MiB      0.0 MiB           1           df = df.withColumn(\"date\", col(\"date\").substr(1, 10))\n",
      "    18     80.0 MiB      0.0 MiB           1           df = df.select(\"date\", col(\"user.username\").alias(\"user\")).dropna()\n",
      "    19                                                 \n",
      "    20                                         \n",
      "    21     80.1 MiB      0.1 MiB           1           date_counts = df.groupBy(\"date\").count().orderBy(desc(\"count\"))\n",
      "    22                                         \n",
      "    23                                         \n",
      "    24     80.1 MiB      0.0 MiB           1           top_10_dates = date_counts.limit(10)\n",
      "    25     80.1 MiB      0.0 MiB           1           joined_df = df.join(top_10_dates, on=\"date\")\n",
      "    26                                         \n",
      "    27                                         # Group by date and user, count occurrences, and use window function to get the most frequent user per date\n",
      "    28     80.1 MiB      0.0 MiB           1           user_counts = joined_df.groupBy(\"date\", \"user\").count()\n",
      "    29     80.1 MiB      0.0 MiB           1           window_spec = Window.partitionBy(\"date\").orderBy(desc(\"count\"))\n",
      "    30     80.1 MiB      0.0 MiB           1           ranked_users = user_counts.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1)\n",
      "    31     80.1 MiB      0.1 MiB           1           result = ranked_users.select(\"date\", \"user\")\n",
      "    32                                                 \n",
      "    33     80.4 MiB      0.0 MiB          14           result_list = [\n",
      "    34     80.4 MiB      0.0 MiB          10               (row['date'], row['user'])\n",
      "    35     80.4 MiB      0.3 MiB           1               for row in result.collect()\n",
      "    36                                                 ]\n",
      "    37     80.4 MiB     -0.0 MiB           1           spark.stop()\n",
      "    38     80.4 MiB      0.0 MiB           1           return(result_list)\n",
      "\n",
      "\n",
      "30.2 s ¬± 0 ns per loop (mean ¬± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from q1_time import q1_time\n",
    "%timeit -r 1 -n 1 result_list = q1_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 550.62890625 MiB\n"
     ]
    }
   ],
   "source": [
    "from memory_profiler import memory_usage\n",
    "from q1_time import q1_time\n",
    "mem_usage = memory_usage((q1_time, (file_path,)), interval=1, timeout=None)\n",
    "print(f\"Memory usage: {max(mem_usage)} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2021-02-18', 'neetuanjle_nitu'), ('2021-02-13', 'MaanDee08215437'), ('2021-02-17', 'RaaJVinderkaur'), ('2021-02-16', 'jot__b'), ('2021-02-20', 'MangalJ23056160'), ('2021-02-14', 'rebelpacifist'), ('2021-02-15', 'jot__b'), ('2021-02-23', 'Surrypuria'), ('2021-02-19', 'Preetm91'), ('2021-02-12', 'RanbirS00614606')]\n"
     ]
    }
   ],
   "source": [
    "result_list = q1_time(file_path)\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Optimizacion del uso Memoria:  Para este caso de aplicacion particular buscando optimizar la memoria se procesara por chunks o fragmentos del archivo inicial esto con la finalidad de tener dataframes que ocupen menos espacio ya que en cada lectura solamente se tendria acumulado un fragmento del archivo en memoria, se evaluaria y se almacena unicamente el resultado de esa transaccion  al final se hace una exploracion conjunta de los resultados. De tal manera que se tendrian los resultados consolidado. Esto requiere mayor tiempo de procesamiento pero el uso de memoria se mantendra bajo. Bajo este mismo concepto se maneja tambien en aplicaciones real time, lo cual podria ser conveniente teniendo en cuenta la plataforma de origen de estos datos, ya que una implementacion como esta nos permitira  leer en tiempo real la informacion que se va generando y al final ir viendo como va cambiando los insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 144.08203125 MiB\n"
     ]
    }
   ],
   "source": [
    "from memory_profiler import memory_usage\n",
    "from q1_memory import q1_memory\n",
    "mem_usage = memory_usage((q1_memory, (file_path,)), interval=1, timeout=None)\n",
    "print(f\"Memory usage: {max(mem_usage)} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.1 s ¬± 0 ns per loop (mean ¬± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from q1_memory import q1_memory\n",
    "%timeit -r 1 -n 1 result_list = q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2021-02-18', 'neetuanjle_nitu'), ('2021-02-13', 'MaanDee08215437'), ('2021-02-17', 'RaaJVinderkaur'), ('2021-02-16', 'jot__b'), ('2021-02-20', 'MangalJ23056160'), ('2021-02-14', 'rebelpacifist'), ('2021-02-15', 'jot__b'), ('2021-02-23', 'Surrypuria'), ('2021-02-19', 'Preetm91'), ('2021-02-12', 'RanbirS00614606')]\n"
     ]
    }
   ],
   "source": [
    "result_list = q1_memory(file_path)\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones: Se ve que en la primera manera se hace en menor tiempo y en la segunda se reduce el consumo de memoria si quisieramos mejorar el uso de memoria hariamos el tamano del fragmento a explorar mas pequeno pero tardariamos mas tiempo. Ahi debemos jugar con los recursos disponibles para tomar una decision en este caso vemos que un tamano de 100 000 lineas no se alarga demasiado el procesamiento "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los top 10 emojis m√°s usados con su respectivo conteo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. En este caso particular se encuentra la posibilidad de que solo se debe leer un valor del json para esto se lee en una lista y luego se crea un solo string para alamacenar todo, esto debido a que no nos interesa saber quien dio la informacion solo que se repita y el procesarlo como un string nos da la ventaja de trabajar con un solo gran bloque de texto sin estructura que nos reducce el procesamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.22 s ¬± 0 ns per loop (mean ¬± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from q2_time import q2_time\n",
    "%timeit -r 1 -n 1 result_list = q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 573.26171875 MiB\n"
     ]
    }
   ],
   "source": [
    "from memory_profiler import memory_usage\n",
    "from q2_time import q2_time\n",
    "mem_usage = memory_usage((q2_time, (file_path,)), interval=1, timeout=None)\n",
    "print(f\"Memory usage: {max(mem_usage)} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 7286), ('üòÇ', 3072), ('üöú', 2972), ('‚úä', 2411), ('üåæ', 2363), ('üèª', 2080), ('‚ù§', 1779), ('ü§£', 1668), ('üèΩ', 1218), ('üëá', 1108)]\n"
     ]
    }
   ],
   "source": [
    "result= q2_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. memory optimization. En este caso se toma el mismo abordaje que en el primero, se hace la division por strings mas pequenos teniendo asi un procesamiento mas pequeno de los datos que se estan manejando.  Siguiendo el mismo concepto de los chunks del punto anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.23 s ¬± 0 ns per loop (mean ¬± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from q2_memory import q2_memory\n",
    "%timeit -r 1 -n 1 result_list = q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 544.1328125 MiB\n"
     ]
    }
   ],
   "source": [
    "from memory_profiler import memory_usage\n",
    "from q2_memory import q2_memory\n",
    "mem_usage = memory_usage((q2_memory, (file_path,)), interval=1, timeout=None)\n",
    "print(f\"Memory usage: {max(mem_usage)} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 7286), ('üòÇ', 3072), ('üöú', 2972), ('‚úä', 2411), ('üåæ', 2363), ('üèª', 2080), ('‚ù§', 1779), ('ü§£', 1668), ('üèΩ', 1218), ('üëá', 1108)]\n"
     ]
    }
   ],
   "source": [
    "from q2_memory import q2_memory\n",
    "result= q2_memory(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones. En este caso al no tener datos que sean de estructura compleja entre una alternativa y otra no se tiene una diferencia significativa como si se noto en el caso anterior. Esto debido a que se estaba trabajando solo con el texto que resulta ser mas ligero de procesar. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
